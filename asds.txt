#!/usr/bin/env python3
import os
import sys
import pandas as pd
import logging
import datetime
from datetime import date
from concurrent.futures import ThreadPoolExecutor
from sqlalchemy import create_engine, inspect, text
import traceback

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("migration.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Configuration - UPDATE THESE VALUES
SOURCE_DB_CONFIG = {
    "host": "source-redshift-cluster.region.redshift.amazonaws.com",
    "port": "5439",
    "database": "source_database",
    "user": "username",
    "password": "password",
    "schema": "source_schema"  # Source schema name
}

TARGET_DB_CONFIG = {
    "host": "target-redshift-cluster.region.redshift.amazonaws.com",
    "port": "5439",
    "database": "target_database",
    "user": "username",
    "password": "password",
    "schema": "target_schema"  # Target schema name
}

# Set this to False to run in sequential mode instead of parallel
USE_PARALLEL = True  

# Number of parallel workers
MAX_WORKERS = 7

# Tables to migrate
TABLES_TO_MIGRATE = [
    "ctms_country_milestones",
    "ctms_site_milestones",
    "ctms_site_visit_details",
    "ctms_study_country_details",
    "ctms_study_details",
    "ctms_study_milestones",
    "ctms_study_site_details"
]

# Create connection engines
def get_connection(db_type):
    """Get a database connection"""
    if db_type == 'source':
        config = SOURCE_DB_CONFIG
    else:
        config = TARGET_DB_CONFIG
        
    conn_uri = f"postgresql://{config['user']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}"
    return create_engine(conn_uri)

def migrate_table(table_name):
    """Migrate a single table from source to target (always append all rows)"""
    start_time = datetime.datetime.now()
    logger.info(f"Starting migration for table {table_name}")
    
    try:
        # Get source and target connections
        source_engine = get_connection('source')
        target_engine = get_connection('target')
        
        # Get source schema name
        source_schema = SOURCE_DB_CONFIG.get('schema')
        target_schema = TARGET_DB_CONFIG.get('schema')
        
        # Build qualified table names
        source_table = f'"{source_schema}"."{table_name}"' if source_schema else f'"{table_name}"'
        target_table = f'"{target_schema}"."{table_name}"' if target_schema else f'"{table_name}"'
        
        # Check if target table exists
        inspector = inspect(target_engine)
        target_tables = inspector.get_table_names(schema=target_schema)
        
        # Extract data from source
        logger.info(f"Extracting data from {source_table}")
        query = f"SELECT * FROM {source_table}"
        
        try:
            # Use safe error handling for row count query
            count_query = f"SELECT COUNT(*) AS row_count FROM {source_table}"
            try:
                count_result = pd.read_sql(count_query, source_engine)
                row_count = count_result.iloc[0]['row_count']
                logger.info(f"Table {table_name} has {row_count} rows to extract")
            except Exception as e:
                logger.warning(f"Could not get row count for {table_name}: {str(e)}")
                row_count = "unknown number of"
            
            # Read the data - use a smaller chunksize for large tables
            df = pd.read_sql(query, source_engine)
            logger.info(f"Extracted {len(df)} rows from {table_name}")
            
            # Add load_date column with today's date
            today_str = date.today().strftime('%Y-%m-%d')
            df['load_date'] = today_str
            
            # Create target table if it doesn't exist, using Redshift-compatible types
            if table_name not in target_tables:
                # For automatic type handling, use to_sql with only a few rows first
                if len(df) > 0:
                    sample_df = df.head(1)
                    sample_df.to_sql(
                        name=table_name,
                        con=target_engine,
                        schema=target_schema,
                        if_exists='replace',
                        index=False
                    )
                    logger.info(f"Created new table {target_table}")
            
            # Upload data to target - always append
            if len(df) > 0:
                # Try to chunk the upload if the dataframe is large
                chunk_size = 10000
                if len(df) > chunk_size:
                    logger.info(f"Uploading {table_name} in chunks...")
                    total_loaded = 0
                    
                    # Upload in chunks
                    for i in range(0, len(df), chunk_size):
                        chunk = df.iloc[i:i+chunk_size]
                        chunk.to_sql(
                            name=table_name,
                            con=target_engine,
                            schema=target_schema,
                            if_exists='append',
                            index=False
                        )
                        total_loaded += len(chunk)
                        logger.info(f"Uploaded chunk {i//chunk_size+1}, total {total_loaded} rows")
                else:
                    # Small table, upload at once
                    df.to_sql(
                        name=table_name,
                        con=target_engine,
                        schema=target_schema,
                        if_exists='append',
                        index=False
                    )
                
                logger.info(f"Successfully loaded {len(df)} rows to {target_table}")
            else:
                logger.warning(f"No data to load for {table_name}")
            
            duration = (datetime.datetime.now() - start_time).total_seconds()
            return {
                "table": table_name,
                "status": "success",
                "rows_loaded": len(df),
                "duration_seconds": duration
            }
        
        except Exception as e:
            logger.error(f"Error processing table {table_name}: {str(e)}")
            logger.error(traceback.format_exc())
            duration = (datetime.datetime.now() - start_time).total_seconds()
            return {
                "table": table_name,
                "status": "error",
                "message": str(e),
                "duration_seconds": duration
            }
    
    except Exception as e:
        logger.error(f"Error migrating table {table_name}: {str(e)}")
        logger.error(traceback.format_exc())
        duration = (datetime.datetime.now() - start_time).total_seconds()
        return {
            "table": table_name,
            "status": "error",
            "message": str(e),
            "duration_seconds": duration
        }

def migrate_tables():
    """Migrate all tables, either in parallel or sequentially based on USE_PARALLEL setting"""
    start_time = datetime.datetime.now()
    today = date.today().strftime('%Y-%m-%d')
    
    if USE_PARALLEL:
        logger.info(f"Starting parallel migration of {len(TABLES_TO_MIGRATE)} tables on {today}")
        results = []
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Submit all tables for migration
            future_to_table = {executor.submit(migrate_table, table): table for table in TABLES_TO_MIGRATE}
            
            # Process results as they complete
            for future in concurrent.futures.as_completed(future_to_table):
                table = future_to_table[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result["status"] == "success":
                        logger.info(f"✓ {table}: {result.get('rows_loaded', 0)} rows in {result.get('duration_seconds', 0):.1f}s")
                    else:
                        logger.error(f"✗ {table}: {result.get('message', 'Unknown error')}")
                except Exception as e:
                    logger.error(f"Error in {table} migration: {str(e)}")
                    results.append({
                        "table": table,
                        "status": "error",
                        "message": str(e)
                    })
    else:
        logger.info(f"Starting sequential migration of {len(TABLES_TO_MIGRATE)} tables on {today}")
        results = []
        
        # Process tables one at a time
        for table in TABLES_TO_MIGRATE:
            result = migrate_table(table)
            results.append(result)
            
            if result["status"] == "success":
                logger.info(f"✓ {table}: {result.get('rows_loaded', 0)} rows in {result.get('duration_seconds', 0):.1f}s")
            else:
                logger.error(f"✗ {table}: {result.get('message', 'Unknown error')}")
    
    return results

def main():
    """Main entry point for the script"""
    try:
        logger.info("=" * 80)
        logger.info("REDSHIFT MIGRATION - APPEND ALL ROWS")
        logger.info("=" * 80)
        logger.info(f"Source: {SOURCE_DB_CONFIG['host']}:{SOURCE_DB_CONFIG['port']}/{SOURCE_DB_CONFIG['database']}/{SOURCE_DB_CONFIG.get('schema')}")
        logger.info(f"Target: {TARGET_DB_CONFIG['host']}:{TARGET_DB_CONFIG['port']}/{TARGET_DB_CONFIG['database']}/{TARGET_DB_CONFIG.get('schema')}")
        logger.info(f"Tables: {', '.join(TABLES_TO_MIGRATE)}")
        logger.info(f"Running in {'parallel' if USE_PARALLEL else 'sequential'} mode")
        if USE_PARALLEL:
            logger.info(f"Max workers: {MAX_WORKERS}")
        logger.info("=" * 80)
        
        # Run the migration
        results = migrate_tables()
        
        # Calculate summary statistics
        successful = [r for r in results if r["status"] == "success"]
        failed = [r for r in results if r["status"] == "error"]
        
        total_rows = sum(r.get('rows_loaded', 0) for r in successful)
        total_duration = (datetime.datetime.now() - start_time).total_seconds()
        
        # Log summary
        logger.info("=" * 80)
        logger.info("MIGRATION SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Total tables: {len(results)}")
        logger.info(f"Successful: {len(successful)}")
        logger.info(f"Failed: {len(failed)}")
        logger.info(f"Total rows loaded: {total_rows}")
        logger.info(f"Total duration: {total_duration:.1f} seconds")
        
        if len(failed) > 0:
            logger.info("Failed tables:")
            for result in failed:
                logger.info(f"  {result['table']}: {result.get('message', 'Unknown error')}")
        
        logger.info("=" * 80)
        
        if len(failed) > 0:
            logger.error("Migration completed with errors.")
            return 1
        else:
            logger.info("Migration completed successfully!")
            return 0
    
    except Exception as e:
        logger.critical(f"Migration failed: {str(e)}")
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    start_time = datetime.datetime.now()
    sys.exit(main())